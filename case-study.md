# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: *измерение потребляемой памяти*

Для начала используя ассимптотку я сделал предположение о том, сколько памяти будет потреблять программа на больших данных.
Для этого я написал простой тест, который запускает программу, передавая ей в качестве аргумента нужный файл

### Результаты (с отключеным GC):
* 12500 строк ~ 104 Mb (530 Mb)
* 25000 строк ~ 137 Mb (1952 Mb)
* 50000 строк ~ 239 Mb (7527 Mb)

Исходя из этих результатов можно сделать вывод что для обработки 3250940 строк нужно чуть менее терабайта памяти (с отключеным GC).
## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *несколько секнуд*

Вот как я построил `feedback_loop`:
1. Написал performance тест для защиты от деградации производительности
2. Написал програмки для запуска различных профилировщиков.
3. Подключил Guard чтобы не тратить время на ручной запуск тестов

Сам `feedback_loop`:
1. запуск профилировщиков и выявление наиболее жирных мест.
1.1. Рефакторинг, если плохо понятно в каком именно месте программы узкое место
1.2. Запуск тестов
1.2. Goto 1
2. Внесение необходимых изменений
3. Запуск тестов
3.1. Откат изменений если тесты не прошли
4. Если метрика не соответствует бюджету - Goto 1

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался *stackprof, ruby-prof, memory-profiler & valgrind*

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- Исходя из бюджета метрики, очевидно что программу надо перписать в потоковом стиле, т.е. обработка и сбор информации должны быть строка за строкой
- Переписал программу в потоковом стиле.
- Метрика улучшилась. При разборе 50000 строк потребляется 42 Mb памяти (вместо 239 Mb ранее)
- Теперь можно переходить к профилированию

### Ваша находка №2
- Для начала сформировал отчет в stackprof. Главная точка роста - Date.parse. То же самое показали и отчеты ruby-prof. Memory profiler, так же указал на эту строку.
- Вместо парсинга даты и затем приведения к формату iso8601, мы можем просто убрать лишний перенос строки в конце. Дата в самом файле лежит уже в нужном нам формате
- Для 95000 записей потребление памяти уменьшилось на 1 Мб с 70 до 69
- Это место перестало быть главной точкой роста.

### Ваша находка №3
- Отчеты memprof и ruby-prof показали главную точку роста - string.split при разбиении строки файла на колонки
- как вы решили её оптимизировать
- как изменилась метрика
- как изменился отчёт профилировщика

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*
